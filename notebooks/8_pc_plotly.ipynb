{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dash\n",
    "from dash import dcc\n",
    "from dash import html\n",
    "from dash.dependencies import Input, Output, State\n",
    "from dash import no_update\n",
    "from dash import dash_table\n",
    "import pandas as pd\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# Matplotlib stuff\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# Same as notebook 7\n",
    "from transformer_lens import utils\n",
    "from sprint.loading import load_all\n",
    "from sprint.linearization import analyze_linearized_feature\n",
    "from sprint.attention import get_attn_head_contribs, get_attn_head_contribs_ov\n",
    "from sprint.sae_tutorial import make_token_df, process_tokens\n",
    "from sprint.visualization import visualize_topk_plotly, plot_attn_contribs_for_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_to_color(val, min_val, max_val):\n",
    "    # Normalize the value\n",
    "    norm = mcolors.Normalize(vmin=min_val, vmax=max_val)\n",
    "    # Choose a colormap\n",
    "    cmap = cm.coolwarm\n",
    "    # Get color from colormap\n",
    "    return mcolors.to_hex(cmap(norm(val)))\n",
    "\n",
    "\n",
    "def generate_style_data_conditional(df):\n",
    "    style_data_conditional = []\n",
    "    for column in df.columns:\n",
    "        if pd.api.types.is_numeric_dtype(df[column]):\n",
    "            min_val = df[column].min()\n",
    "            max_val = df[column].max()\n",
    "            column_styles = [\n",
    "                {\n",
    "                    \"if\": {\"filter_query\": f\"{{{column}}} eq {val}\", \"column_id\": column},\n",
    "                    \"backgroundColor\": value_to_color(val, min_val, max_val),\n",
    "                    \"color\": \"black\",  # or 'white' depending on contrast\n",
    "                }\n",
    "                for val in df[column].unique()\n",
    "            ]\n",
    "            style_data_conditional.extend(column_styles)\n",
    "    return style_data_conditional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame object at 0x7fefd05b5330>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gelu-2l into HookedTransformer\n",
      "Moving model to device:  cuda\n",
      "Changing model dtype to torch.float16\n",
      "Model device: cuda:0\n",
      "Tokens shape: torch.Size([215402, 128]), dtype: torch.int64, device: cuda:0\n",
      "{'act_name': 'blocks.1.hook_mlp_out',\n",
      " 'act_size': 512,\n",
      " 'batch_size': 4096,\n",
      " 'beta1': 0.9,\n",
      " 'beta2': 0.99,\n",
      " 'buffer_batches': 12288,\n",
      " 'buffer_mult': 384,\n",
      " 'buffer_size': 1572864,\n",
      " 'device': 'cuda:0',\n",
      " 'dict_mult': 32,\n",
      " 'dict_size': 16384,\n",
      " 'enc_dtype': 'fp32',\n",
      " 'l1_coeff': 0.0003,\n",
      " 'layer': 1,\n",
      " 'lr': 0.0001,\n",
      " 'model_batch_size': 512,\n",
      " 'model_name': 'gelu-2l',\n",
      " 'num_tokens': 2000000000,\n",
      " 'remove_rare_dir': False,\n",
      " 'seed': 50,\n",
      " 'seq_len': 128,\n",
      " 'site': 'mlp_out'}\n",
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)\n",
      "File \u001b[1;32m~/mats-sprint/src/sprint/loading.py:56\u001b[0m, in \u001b[0;36mload_all\u001b[1;34m(\n",
      "    **kwargs={'model_name': 'gelu-2l', 'run_id': 'l1'}\n",
      ")\u001b[0m\n",
      "\u001b[0;32m     54\u001b[0m model \u001b[38;5;241m=\u001b[39m load_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;32m     55\u001b[0m tokens \u001b[38;5;241m=\u001b[39m load_data(model\u001b[38;5;241m=\u001b[39mmodel, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;32m---> 56\u001b[0m sae \u001b[38;5;241m=\u001b[39m \u001b[43mload_sae\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "        kwargs \u001b[1;34m= {'model_name': 'gelu-2l', 'run_id': 'l1'}\u001b[0m\n",
      "\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, tokens, sae\n",
      "\n",
      "File \u001b[1;32m~/mats-sprint/src/sprint/loading.py:46\u001b[0m, in \u001b[0;36mload_sae\u001b[1;34m(\n",
      "    run_id='l1',\n",
      "    use_cuda=True,\n",
      "    half_precision=True,\n",
      "    verbose=True,\n",
      "    **kwargs={'model_name': 'gelu-2l'}\n",
      ")\u001b[0m\n",
      "\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_sae\u001b[39m(\n",
      "\u001b[0;32m     44\u001b[0m     run_id: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m RUN, use_cuda: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, half_precision: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, verbose: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n",
      "\u001b[0;32m     45\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AutoEncoder:\n",
      "\u001b[1;32m---> 46\u001b[0m     encoder \u001b[38;5;241m=\u001b[39m \u001b[43mAutoEncoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_hf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "        run_id \u001b[1;34m= 'l1'\u001b[0m\n",
      "\u001b[0;32m     47\u001b[0m     encoder \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mcuda() \u001b[38;5;28;01mif\u001b[39;00m use_cuda \u001b[38;5;28;01melse\u001b[39;00m encoder\n",
      "\u001b[0;32m     48\u001b[0m     model \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat16) \u001b[38;5;28;01mif\u001b[39;00m half_precision \u001b[38;5;28;01melse\u001b[39;00m model\n",
      "\n",
      "File \u001b[1;32m~/mats-sprint/src/sprint/sae_tutorial.py:106\u001b[0m, in \u001b[0;36mAutoEncoder.load_from_hf\u001b[1;34m(\n",
      "    cls=<class 'sprint.sae_tutorial.AutoEncoder'>,\n",
      "    version='l1'\n",
      ")\u001b[0m\n",
      "\u001b[0;32m    104\u001b[0m     cfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124md_mlp\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2048\u001b[39m  \u001b[38;5;66;03m# This was not encoded for some reason\u001b[39;00m\n",
      "\u001b[0;32m    105\u001b[0m pprint\u001b[38;5;241m.\u001b[39mpprint(cfg)\n",
      "\u001b[1;32m--> 106\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "        cfg \u001b[1;34m= {'seed': 50, 'batch_size': 4096, 'buffer_mult': 384, 'lr': 0.0001, 'num_tokens': 2000000000, 'l1_coeff': 0.0003, 'beta1': 0.9, 'beta2': 0.99, 'dict_mult': 32, 'seq_len': 128, 'enc_dtype': 'fp32', 'remove_rare_dir': False, 'model_name': 'gelu-2l', 'site': 'mlp_out', 'layer': 1, 'device': 'cuda:0', 'model_batch_size': 512, 'buffer_size': 1572864, 'buffer_batches': 12288, 'act_name': 'blocks.1.hook_mlp_out', 'act_size': 512, 'dict_size': 16384}\u001b[0m\u001b[1;34m\n",
      "        \u001b[0mcls \u001b[1;34m= <class 'sprint.sae_tutorial.AutoEncoder'>\u001b[0m\n",
      "\u001b[0;32m    107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_state_dict(\n",
      "\u001b[0;32m    108\u001b[0m     utils\u001b[38;5;241m.\u001b[39mdownload_file_from_hf(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNeelNanda/sparse_autoencoder\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhf_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, force_is_torch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;32m    109\u001b[0m )\n",
      "\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\n",
      "File \u001b[1;32m~/mats-sprint/src/sprint/sae_tutorial.py:34\u001b[0m, in \u001b[0;36mAutoEncoder.__init__\u001b[1;34m(\n",
      "    self=AutoEncoder(),\n",
      "    cfg={'act_name': 'blocks.1.hook_mlp_out', 'act_size': 512, 'batch_size': 4096, 'beta1': 0.9, 'beta2': 0.99, 'buffer_batches': 12288, 'buffer_mult': 384, 'buffer_size': 1572864, 'device': 'cuda:0', 'dict_mult': 32, ...}\n",
      ")\u001b[0m\n",
      "\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, cfg):\n",
      "\u001b[0;32m     33\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;32m---> 34\u001b[0m     d_hidden \u001b[38;5;241m=\u001b[39m \u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43md_mlp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m*\u001b[39m cfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdict_mult\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "        cfg[\"dict_mult\"] \u001b[1;34m= 32\u001b[0m\u001b[1;34m\n",
      "        \u001b[0mcfg \u001b[1;34m= {'seed': 50, 'batch_size': 4096, 'buffer_mult': 384, 'lr': 0.0001, 'num_tokens': 2000000000, 'l1_coeff': 0.0003, 'beta1': 0.9, 'beta2': 0.99, 'dict_mult': 32, 'seq_len': 128, 'enc_dtype': 'fp32', 'remove_rare_dir': False, 'model_name': 'gelu-2l', 'site': 'mlp_out', 'layer': 1, 'device': 'cuda:0', 'model_batch_size': 512, 'buffer_size': 1572864, 'buffer_batches': 12288, 'act_name': 'blocks.1.hook_mlp_out', 'act_size': 512, 'dict_size': 16384}\u001b[0m\n",
      "\u001b[0;32m     35\u001b[0m     d_mlp \u001b[38;5;241m=\u001b[39m cfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124md_mlp\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;32m     36\u001b[0m     l1_coeff \u001b[38;5;241m=\u001b[39m cfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml1_coeff\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\n",
      "\u001b[1;31mKeyError\u001b[0m: 'd_mlp'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Dash app\n",
    "\n",
    "\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "# Define your layour\n",
    "app.layout = html.Div(\n",
    "    [\n",
    "        # Model and data loading section\n",
    "        html.Div(\n",
    "            [\n",
    "                html.Div(\n",
    "                    [\n",
    "                        html.Label(\"Model + SAE:\"),\n",
    "                        dcc.Dropdown(\n",
    "                            id=\"model_sae_name\",\n",
    "                            options=[\n",
    "                                {\"label\": \"gelu-1l, run 1\", \"value\": \"gelu-1l, run 1\"},\n",
    "                                {\"label\": \"gelu-1l, run 2\", \"value\": \"gelu-1l, run 2\"},\n",
    "                                {\"label\": \"gelu-2l, layer 0\", \"value\": \"gelu-2l, layer 0\"},\n",
    "                                {\"label\": \"gelu-2l, layer 1\", \"value\": \"gelu-2l, layer 1\"},\n",
    "                            ],\n",
    "                            style={\"width\": \"50%\"},  # Adjust width here\n",
    "                            value=\"gelu-1l, run 1\",\n",
    "                        ),\n",
    "                    ],\n",
    "                    style={\"margin-bottom\": \"10px\"},  # Adjust spacing as needed\n",
    "                ),\n",
    "                html.Div(\n",
    "                    [\n",
    "                        html.Button(\"Load model\", id=\"load-button\", n_clicks=0),\n",
    "                    ],\n",
    "                    style={\"margin-bottom\": \"10px\"},\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "        # Feature, sample, token, attention head callback happens here\n",
    "        html.Div(\n",
    "            [\n",
    "                html.Div(\n",
    "                    [\n",
    "                        html.Label(\"Feature ID:\"),\n",
    "                        dcc.Input(id=\"feature-id\", type=\"text\", value=\"4542\"),\n",
    "                    ],\n",
    "                    style={\"margin-bottom\": \"10px\"},\n",
    "                ),  # Add margin for spacing\n",
    "                html.Div(\n",
    "                    [\n",
    "                        html.Label(\"Sample #:\"),\n",
    "                        dcc.Input(id=\"sample-idx\", type=\"text\", value=\"38\"),\n",
    "                    ],\n",
    "                    style={\"margin-bottom\": \"10px\"},\n",
    "                ),\n",
    "                html.Div(\n",
    "                    [\n",
    "                        html.Label(\"Token #:\"),\n",
    "                        dcc.Input(id=\"token-idx\", type=\"text\", value=\"73\"),\n",
    "                    ],\n",
    "                    style={\"margin-bottom\": \"10px\"},\n",
    "                ),\n",
    "                html.Div(\n",
    "                    [\n",
    "                        html.Label(\"Attention Head:\"),\n",
    "                        dcc.Dropdown(\n",
    "                            id=\"attention-head\",\n",
    "                            options=[{\"label\": str(i), \"value\": i} for i in range(8)],\n",
    "                            style={\"width\": \"50%\"},  # Adjust width here\n",
    "                            value=0,\n",
    "                        ),\n",
    "                    ],\n",
    "                    style={\"margin-bottom\": \"10px\"},\n",
    "                ),\n",
    "                html.Div(\n",
    "                    [\n",
    "                        html.Label(\"Batch size\"),\n",
    "                        dcc.Input(id=\"batch-size\", type=\"text\", value=\"64\"),\n",
    "                    ],\n",
    "                    style={\"margin-bottom\": \"10px\"},\n",
    "                ),\n",
    "                html.Button(\"Update\", id=\"update-button\", n_clicks=0),\n",
    "            ],\n",
    "            style={\"width\": \"300px\", \"margin\": \"auto\"},\n",
    "        ),  # Adjust the overall width of the input container\n",
    "        html.Div(id=\"model-name\", children=[]),\n",
    "        html.Div(id=\"output-container\", children=[]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize your data and model\n",
    "@app.callback(\n",
    "    Output(\"model-name\", \"children\"),\n",
    "    [Input(\"load-button\", \"n_clicks\")],\n",
    "    [State(\"model_sae_name\", \"value\")],\n",
    ")\n",
    "def load_model(n_clicks, model_sae_name):\n",
    "    if n_clicks < 1:\n",
    "        return []\n",
    "\n",
    "    global model, data, sae, sae_layer\n",
    "    if model_sae_name == \"gelu-1l, run 1\":\n",
    "        model_name = \"gelu-1l\"\n",
    "        sae_name = \"run1\"\n",
    "        sae_layer = 0\n",
    "    elif model_sae_name == \"gelu-1l, run 2\":\n",
    "        model_name = \"gelu-1l\"\n",
    "        sae_name = \"run2\"\n",
    "        sae_layer = 0\n",
    "    elif model_sae_name == \"gelu-2l, layer 0\":\n",
    "        model_name = \"gelu-2l\"\n",
    "        sae_name = \"l0\"\n",
    "        sae_layer = 0\n",
    "    elif model_sae_name == \"gelu-2l, layer 1\":\n",
    "        model_name = \"gelu-2l\"\n",
    "        sae_name = \"l1\"\n",
    "        sae_layer = 1\n",
    "\n",
    "    model, data, sae = load_all(model_name=model_name, run_id=sae_name)\n",
    "    return [html.Div(f\"Loaded model: {model_name}, SAE: {sae_name}, SAE Layer: {sae_layer}\")]\n",
    "\n",
    "\n",
    "# Define the callback for updating outputs\n",
    "@app.callback(\n",
    "    Output(\"output-container\", \"children\"),\n",
    "    [Input(\"update-button\", \"n_clicks\")],\n",
    "    [\n",
    "        State(\"feature-id\", \"value\"),\n",
    "        State(\"sample-idx\", \"value\"),\n",
    "        State(\"token-idx\", \"value\"),\n",
    "        State(\"attention-head\", \"value\"),\n",
    "        State(\"batch-size\", \"value\"),\n",
    "    ],\n",
    ")\n",
    "def update_output(n_clicks, feature_id, sample_idx, token_idx, attention_head, batch_size):\n",
    "    if n_clicks < 1:\n",
    "        return []\n",
    "\n",
    "    # Placeholder for PyTorch computation\n",
    "    # result = run_pytorch_computation(feature_id, sample_num, token_num, attention_head)\n",
    "    result = analyze_linearized_feature(\n",
    "        feature_idx=int(feature_id),\n",
    "        sample_idx=int(sample_idx),\n",
    "        token_idx=int(token_idx),\n",
    "        model=model,\n",
    "        data=data,\n",
    "        encoder=sae,\n",
    "        head=int(attention_head),\n",
    "        batch_size=int(batch_size),\n",
    "    )\n",
    "    batch = data[: int(batch_size)]\n",
    "\n",
    "    # Dataframes\n",
    "    df1 = make_token_df(batch, model=model)\n",
    "    df1[\"feature\"] = utils.to_numpy(result[\"sae activations\"][:, int(feature_id)])\n",
    "    df1 = df1.sort_values(\"feature\", ascending=False).head(20)\n",
    "\n",
    "    df2 = make_token_df(batch, model=model)\n",
    "    df2[\"feature\"] = utils.to_numpy(result[\"activation scores\"])\n",
    "    df2 = df2.sort_values(\"feature\", ascending=False).head(20)\n",
    "\n",
    "    df3 = pd.DataFrame(\n",
    "        dict(str_tokens=result[\"token strings\"], feature_scores=result[\"token scores\"].detach().cpu().numpy())\n",
    "    )\n",
    "\n",
    "    df_list = [df1, df2, df3]\n",
    "    df_titles = [\"SAE Activations\", \"Activation Scores\", \"Token Scores\"]\n",
    "    # df_list = [df3]\n",
    "\n",
    "    # Plots\n",
    "    # plot1 = visualize_topk(feature_id=feature_id, n_examples=10, model=model, pad=True, clip=10)\n",
    "    plot1 = visualize_topk_plotly(feature_id=feature_id, n_examples=10, model=model, pad=True, clip=10)\n",
    "    plot2 = plot_attn_contribs_for_example(\n",
    "        model=model,\n",
    "        data=data,\n",
    "        example_idx=int(sample_idx),\n",
    "        token_idx=int(token_idx),\n",
    "        feature_mid=result[\"mid\"],\n",
    "        ov_only=False,\n",
    "        batch_size=int(batch_size),\n",
    "    )\n",
    "    # plot_list = [plot1, plot2]\n",
    "    plot_list = [plot1]\n",
    "\n",
    "    # Generate Data Tables\n",
    "    data_tables = [\n",
    "        (\n",
    "            html.Div(title),\n",
    "            dash_table.DataTable(\n",
    "                data=df.to_dict(\"records\"),\n",
    "                columns=[{\"name\": i, \"id\": i} for i in df.columns],\n",
    "                style_cell={\n",
    "                    \"minWidth\": \"50px\",\n",
    "                    \"width\": \"50px\",\n",
    "                    \"maxWidth\": \"50px\",  # Adjust the width as needed\n",
    "                    \"textAlign\": \"center\",\n",
    "                },\n",
    "                style_table={\"maxHeight\": \"300px\", \"overflowY\": \"scroll\", \"margin\": \"auto\"},\n",
    "                style_data_conditional=generate_style_data_conditional(df),\n",
    "            ),\n",
    "        )\n",
    "        for df, title in zip(df_list, df_titles)\n",
    "    ]\n",
    "    data_tables = [item for sublist in data_tables for item in sublist]  # Flatten list\n",
    "\n",
    "    # Generate Plots\n",
    "    plots = [dcc.Graph(figure=plot) for plot in plot_list]\n",
    "\n",
    "    # Combine all elements for display\n",
    "    return data_tables + plots\n",
    "\n",
    "\n",
    "app.run(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'act_name': 'blocks.1.hook_mlp_out',\n",
      " 'act_size': 512,\n",
      " 'batch_size': 4096,\n",
      " 'beta1': 0.9,\n",
      " 'beta2': 0.99,\n",
      " 'buffer_batches': 12288,\n",
      " 'buffer_mult': 384,\n",
      " 'buffer_size': 1572864,\n",
      " 'device': 'cuda:0',\n",
      " 'dict_mult': 32,\n",
      " 'dict_size': 16384,\n",
      " 'enc_dtype': 'fp32',\n",
      " 'l1_coeff': 0.0003,\n",
      " 'layer': 1,\n",
      " 'lr': 0.0001,\n",
      " 'model_batch_size': 512,\n",
      " 'model_name': 'gelu-2l',\n",
      " 'num_tokens': 2000000000,\n",
      " 'remove_rare_dir': False,\n",
      " 'seed': 50,\n",
      " 'seq_len': 128,\n",
      " 'site': 'mlp_out'}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'d_mlp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msprint\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloading\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_sae\n\u001b[0;32m----> 3\u001b[0m \u001b[43mload_sae\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ml1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mats-sprint/src/sprint/loading.py:46\u001b[0m, in \u001b[0;36mload_sae\u001b[0;34m(run_id, use_cuda, half_precision, verbose, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_sae\u001b[39m(\n\u001b[1;32m     44\u001b[0m     run_id: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m RUN, use_cuda: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, half_precision: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, verbose: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m     45\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AutoEncoder:\n\u001b[0;32m---> 46\u001b[0m     encoder \u001b[38;5;241m=\u001b[39m \u001b[43mAutoEncoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_hf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     encoder \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mcuda() \u001b[38;5;28;01mif\u001b[39;00m use_cuda \u001b[38;5;28;01melse\u001b[39;00m encoder\n\u001b[1;32m     48\u001b[0m     model \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat16) \u001b[38;5;28;01mif\u001b[39;00m half_precision \u001b[38;5;28;01melse\u001b[39;00m model\n",
      "File \u001b[0;32m~/mats-sprint/src/sprint/sae_tutorial.py:106\u001b[0m, in \u001b[0;36mAutoEncoder.load_from_hf\u001b[0;34m(cls, version)\u001b[0m\n\u001b[1;32m    104\u001b[0m     cfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124md_mlp\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2048\u001b[39m  \u001b[38;5;66;03m# This was not encoded for some reason\u001b[39;00m\n\u001b[1;32m    105\u001b[0m pprint\u001b[38;5;241m.\u001b[39mpprint(cfg)\n\u001b[0;32m--> 106\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_state_dict(\n\u001b[1;32m    108\u001b[0m     utils\u001b[38;5;241m.\u001b[39mdownload_file_from_hf(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNeelNanda/sparse_autoencoder\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhf_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, force_is_torch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    109\u001b[0m )\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/mats-sprint/src/sprint/sae_tutorial.py:34\u001b[0m, in \u001b[0;36mAutoEncoder.__init__\u001b[0;34m(self, cfg)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, cfg):\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m---> 34\u001b[0m     d_hidden \u001b[38;5;241m=\u001b[39m \u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43md_mlp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m*\u001b[39m cfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdict_mult\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     35\u001b[0m     d_mlp \u001b[38;5;241m=\u001b[39m cfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124md_mlp\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     36\u001b[0m     l1_coeff \u001b[38;5;241m=\u001b[39m cfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml1_coeff\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'd_mlp'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)\n",
      "File \u001b[1;32m~/mats-sprint/src/sprint/linearization.py:67\u001b[0m, in \u001b[0;36manalyze_linearized_feature\u001b[1;34m(\n",
      "    feature_idx=4542,\n",
      "    sample_idx=38,\n",
      "    token_idx=73,\n",
      "    layer=0,\n",
      "    head=0,\n",
      "    model=HookedTransformer(\n",
      "  (embed): Embed()\n",
      "  (hook_em...alized): HookPoint()\n",
      "  )\n",
      "  (unembed): Unembed()\n",
      "),\n",
      "    encoder=AutoEncoder(),\n",
      "    data=tensor([[    1,    10, 27498,  ...,    65, 32489...25,  ...,  5181,    65,  3313]], device='cuda:0'),\n",
      "    batch_size=64,\n",
      "    n_tokens=10\n",
      ")\u001b[0m\n",
      "\u001b[0;32m     65\u001b[0m mlp_acts \u001b[38;5;241m=\u001b[39m cache[utils\u001b[38;5;241m.\u001b[39mget_act_name(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, layer)]\n",
      "\u001b[0;32m     66\u001b[0m mlp_acts_flattened \u001b[38;5;241m=\u001b[39m mlp_acts\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, SAE_CFG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124md_mlp\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;32m---> 67\u001b[0m _, _, hidden_acts, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmlp_acts_flattened\u001b[49m\u001b[43m)\u001b[49m\n",
      "        mlp_acts_flattened \u001b[1;34m= tensor([[-1.6870e-01, -4.2419e-02, -3.3617e-05,  ..., -8.9294e-02,\n",
      "         -6.6895e-02, -1.3208e-03],\n",
      "        [-1.6406e-01, -1.2999e-03, -8.0643e-03,  ..., -8.4961e-02,\n",
      "         -1.3098e-01, -1.3562e-01],\n",
      "        [ 2.3926e-01,  1.1310e-01, -9.0027e-02,  ..., -2.0432e-02,\n",
      "         -8.2214e-02, -1.6199e-01],\n",
      "        ...,\n",
      "        [ 1.8201e-01, -1.6528e-01, -1.6638e-01,  ..., -6.0913e-02,\n",
      "          3.9893e-01, -4.7394e-02],\n",
      "        [-1.3147e-01, -1.6968e-01, -6.2927e-02,  ..., -1.0144e-01,\n",
      "         -8.1177e-02, -7.3059e-02],\n",
      "        [ 4.4250e-02, -1.5686e-01, -9.2834e-02,  ..., -1.6406e-01,\n",
      "          8.0994e-02,  1.1688e-01]], device='cuda:0', dtype=torch.float16)\u001b[0m\u001b[1;34m\n",
      "        \u001b[0mencoder \u001b[1;34m= AutoEncoder()\u001b[0m\u001b[1;34m\n",
      "        \u001b[0m_ \u001b[1;34m= tensor([[[ 6.3125, -6.1367, -6.1406,  ..., -6.1445, -6.1406, -6.1406],\n",
      "         [ 4.3711, -5.4766, -5.4727,  ..., -5.5156, -5.4609, -5.4648],\n",
      "         [ 5.7266, -4.9961, -5.0156,  ..., -4.9805, -5.0078, -5.0039],\n",
      "         ...,\n",
      "         [ 5.2891, -4.4531, -4.5391,  ..., -4.5000, -4.5234, -4.5469],\n",
      "         [ 9.4844, -5.0547, -5.1367,  ..., -5.0625, -5.1172, -5.0781],\n",
      "         [ 1.8418, -5.6641, -5.6953,  ..., -5.7031, -5.6836, -5.7031]],\n",
      "\n",
      "        [[ 6.3125, -6.1367, -6.1406,  ..., -6.1445, -6.1406, -6.1406],\n",
      "         [ 4.2578, -5.7500, -5.7266,  ..., -5.7656, -5.7734, -5.7578],\n",
      "         [ 5.6211, -6.6172, -6.5312,  ..., -6.5938, -6.6016, -6.5703],\n",
      "         ...,\n",
      "         [ 5.3789, -8.0234, -7.9766,  ..., -7.9609, -7.9609, -7.9844],\n",
      "         [ 3.5156, -7.9844, -7.9766,  ..., -7.9453, -7.9531, -8.0625],\n",
      "         [ 4.0469, -7.7500, -7.7031,  ..., -7.6875, -7.7109, -7.7227]],\n",
      "\n",
      "        [[ 6.3125, -6.1367, -6.1406,  ..., -6.1445, -6.1406, -6.1406],\n",
      "         [ 6.9492, -4.9375, -4.9492,  ..., -4.9531, -4.9336, -4.9102],\n",
      "         [ 1.1035, -4.7812, -4.7422,  ..., -4.7656, -4.7656, -4.7812],\n",
      "         ...,\n",
      "         [ 5.2344, -5.7148, -5.6914,  ..., -5.7422, -5.7578, -5.7031],\n",
      "         [ 1.7607, -5.5156, -5.5664,  ..., -5.5156, -5.5156, -5.5156],\n",
      "         [ 3.4922, -5.0312, -5.0273,  ..., -5.0625, -5.0391, -4.9688]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 6.3125, -6.1367, -6.1406,  ..., -6.1445, -6.1406, -6.1406],\n",
      "         [ 5.0234, -5.5430, -5.5781,  ..., -5.5586, -5.5391, -5.4883],\n",
      "         [ 8.4688, -6.3516, -6.3281,  ..., -6.3047, -6.3984, -6.3516],\n",
      "         ...,\n",
      "         [ 3.7617, -6.6719, -6.6406,  ..., -6.5625, -6.6875, -6.6328],\n",
      "         [-1.2324, -5.1484, -5.1953,  ..., -5.1797, -5.2422, -5.2422],\n",
      "         [ 1.2988, -3.9062, -3.8242,  ..., -3.9512, -3.9062, -4.0469]],\n",
      "\n",
      "        [[ 6.3125, -6.1367, -6.1406,  ..., -6.1445, -6.1406, -6.1406],\n",
      "         [ 2.1914, -6.0898, -6.0703,  ..., -6.0781, -6.0938, -6.0781],\n",
      "         [ 4.5625, -7.6094, -7.6094,  ..., -7.5938, -7.6094, -7.5938],\n",
      "         ...,\n",
      "         [ 2.6914, -7.5000, -7.4219,  ..., -7.3828, -7.4531, -7.4570],\n",
      "         [-3.8691, -8.1094, -8.0078,  ..., -8.0312, -8.0078, -8.2031],\n",
      "         [16.4219, -6.9531, -6.9297,  ..., -6.8984, -6.8750, -6.9492]],\n",
      "\n",
      "        [[ 6.3125, -6.1367, -6.1406,  ..., -6.1445, -6.1406, -6.1406],\n",
      "         [ 6.7070, -5.5859, -5.6172,  ..., -5.6406, -5.6016, -5.6094],\n",
      "         [ 3.4551, -6.7500, -6.7891,  ..., -6.7656, -6.7656, -6.7305],\n",
      "         ...,\n",
      "         [ 2.9434, -4.2656, -4.3047,  ..., -4.2656, -4.2656, -4.2812],\n",
      "         [ 4.7109, -5.2812, -5.3125,  ..., -5.2500, -5.2773, -5.2148],\n",
      "         [ 3.9668, -4.8984, -4.9375,  ..., -4.8164, -4.8672, -4.8203]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>)\u001b[0m\n",
      "\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Linearization component\u001b[39;00m\n",
      "\u001b[0;32m     70\u001b[0m feature \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mW_enc[:, feature_idx]\n",
      "\n",
      "File \u001b[1;32m/opt/conda/envs/sprint/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(\n",
      "    self=AutoEncoder(),\n",
      "    *args=(tensor([[-1.6870e-01, -4.2419e-02, -3.3617e-05, ...1688e-01]], device='cuda:0', dtype=torch.float16),),\n",
      "    **kwargs={}\n",
      ")\u001b[0m\n",
      "\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "        self \u001b[1;34m= AutoEncoder()\u001b[0m\u001b[1;34m\n",
      "        \u001b[0margs \u001b[1;34m= (tensor([[-1.6870e-01, -4.2419e-02, -3.3617e-05,  ..., -8.9294e-02,\n",
      "         -6.6895e-02, -1.3208e-03],\n",
      "        [-1.6406e-01, -1.2999e-03, -8.0643e-03,  ..., -8.4961e-02,\n",
      "         -1.3098e-01, -1.3562e-01],\n",
      "        [ 2.3926e-01,  1.1310e-01, -9.0027e-02,  ..., -2.0432e-02,\n",
      "         -8.2214e-02, -1.6199e-01],\n",
      "        ...,\n",
      "        [ 1.8201e-01, -1.6528e-01, -1.6638e-01,  ..., -6.0913e-02,\n",
      "          3.9893e-01, -4.7394e-02],\n",
      "        [-1.3147e-01, -1.6968e-01, -6.2927e-02,  ..., -1.0144e-01,\n",
      "         -8.1177e-02, -7.3059e-02],\n",
      "        [ 4.4250e-02, -1.5686e-01, -9.2834e-02,  ..., -1.6406e-01,\n",
      "          8.0994e-02,  1.1688e-01]], device='cuda:0', dtype=torch.float16),)\u001b[0m\u001b[1;34m\n",
      "        \u001b[0mkwargs \u001b[1;34m= {}\u001b[0m\n",
      "\n",
      "File \u001b[1;32m/opt/conda/envs/sprint/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(\n",
      "    self=AutoEncoder(),\n",
      "    *args=(tensor([[-1.6870e-01, -4.2419e-02, -3.3617e-05, ...1688e-01]], device='cuda:0', dtype=torch.float16),),\n",
      "    **kwargs={}\n",
      ")\u001b[0m\n",
      "\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "        forward_call \u001b[1;34m= <bound method AutoEncoder.forward of AutoEncoder()>\u001b[0m\u001b[1;34m\n",
      "        \u001b[0margs \u001b[1;34m= (tensor([[-1.6870e-01, -4.2419e-02, -3.3617e-05,  ..., -8.9294e-02,\n",
      "         -6.6895e-02, -1.3208e-03],\n",
      "        [-1.6406e-01, -1.2999e-03, -8.0643e-03,  ..., -8.4961e-02,\n",
      "         -1.3098e-01, -1.3562e-01],\n",
      "        [ 2.3926e-01,  1.1310e-01, -9.0027e-02,  ..., -2.0432e-02,\n",
      "         -8.2214e-02, -1.6199e-01],\n",
      "        ...,\n",
      "        [ 1.8201e-01, -1.6528e-01, -1.6638e-01,  ..., -6.0913e-02,\n",
      "          3.9893e-01, -4.7394e-02],\n",
      "        [-1.3147e-01, -1.6968e-01, -6.2927e-02,  ..., -1.0144e-01,\n",
      "         -8.1177e-02, -7.3059e-02],\n",
      "        [ 4.4250e-02, -1.5686e-01, -9.2834e-02,  ..., -1.6406e-01,\n",
      "          8.0994e-02,  1.1688e-01]], device='cuda:0', dtype=torch.float16),)\u001b[0m\u001b[1;34m\n",
      "        \u001b[0mkwargs \u001b[1;34m= {}\u001b[0m\n",
      "\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32m~/mats-sprint/src/sprint/sae_tutorial.py:52\u001b[0m, in \u001b[0;36mAutoEncoder.forward\u001b[1;34m(\n",
      "    self=AutoEncoder(),\n",
      "    x=tensor([[-1.6870e-01, -4.2419e-02, -3.3617e-05, ...1688e-01]], device='cuda:0', dtype=torch.float16)\n",
      ")\u001b[0m\n",
      "\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n",
      "\u001b[1;32m---> 52\u001b[0m     x_cent \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mb_dec\u001b[49m\n",
      "        x \u001b[1;34m= tensor([[-1.6870e-01, -4.2419e-02, -3.3617e-05,  ..., -8.9294e-02,\n",
      "         -6.6895e-02, -1.3208e-03],\n",
      "        [-1.6406e-01, -1.2999e-03, -8.0643e-03,  ..., -8.4961e-02,\n",
      "         -1.3098e-01, -1.3562e-01],\n",
      "        [ 2.3926e-01,  1.1310e-01, -9.0027e-02,  ..., -2.0432e-02,\n",
      "         -8.2214e-02, -1.6199e-01],\n",
      "        ...,\n",
      "        [ 1.8201e-01, -1.6528e-01, -1.6638e-01,  ..., -6.0913e-02,\n",
      "          3.9893e-01, -4.7394e-02],\n",
      "        [-1.3147e-01, -1.6968e-01, -6.2927e-02,  ..., -1.0144e-01,\n",
      "         -8.1177e-02, -7.3059e-02],\n",
      "        [ 4.4250e-02, -1.5686e-01, -9.2834e-02,  ..., -1.6406e-01,\n",
      "          8.0994e-02,  1.1688e-01]], device='cuda:0', dtype=torch.float16)\u001b[0m\u001b[1;34m\n",
      "        \u001b[0mself \u001b[1;34m= AutoEncoder()\u001b[0m\n",
      "\u001b[0;32m     53\u001b[0m     acts \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x_cent \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_enc \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb_enc)\n",
      "\u001b[0;32m     54\u001b[0m     x_reconstruct \u001b[38;5;241m=\u001b[39m acts \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_dec \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb_dec\n",
      "\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (2048) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "\n",
      "Loaded pretrained model gelu-1l into HookedTransformer\n",
      "Moving model to device:  cuda\n",
      "Changing model dtype to torch.float16\n",
      "Model device: cuda:0\n",
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)\n",
      "File \u001b[1;32m~/mats-sprint/src/sprint/linearization.py:67\u001b[0m, in \u001b[0;36manalyze_linearized_feature\u001b[1;34m(\n",
      "    feature_idx=4542,\n",
      "    sample_idx=38,\n",
      "    token_idx=73,\n",
      "    layer=0,\n",
      "    head=0,\n",
      "    model=HookedTransformer(\n",
      "  (embed): Embed()\n",
      "  (hook_em...alized): HookPoint()\n",
      "  )\n",
      "  (unembed): Unembed()\n",
      "),\n",
      "    encoder=AutoEncoder(),\n",
      "    data=tensor([[    1,    10, 27498,  ...,    65, 32489...25,  ...,  5181,    65,  3313]], device='cuda:0'),\n",
      "    batch_size=64,\n",
      "    n_tokens=10\n",
      ")\u001b[0m\n",
      "\u001b[0;32m     65\u001b[0m mlp_acts \u001b[38;5;241m=\u001b[39m cache[utils\u001b[38;5;241m.\u001b[39mget_act_name(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, layer)]\n",
      "\u001b[0;32m     66\u001b[0m mlp_acts_flattened \u001b[38;5;241m=\u001b[39m mlp_acts\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, SAE_CFG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124md_mlp\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;32m---> 67\u001b[0m _, _, hidden_acts, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmlp_acts_flattened\u001b[49m\u001b[43m)\u001b[49m\n",
      "        mlp_acts_flattened \u001b[1;34m= tensor([[-1.6870e-01, -4.2419e-02, -3.3617e-05,  ..., -8.9294e-02,\n",
      "         -6.6895e-02, -1.3208e-03],\n",
      "        [-1.6406e-01, -1.2999e-03, -8.0643e-03,  ..., -8.4961e-02,\n",
      "         -1.3098e-01, -1.3562e-01],\n",
      "        [ 2.3926e-01,  1.1310e-01, -9.0027e-02,  ..., -2.0432e-02,\n",
      "         -8.2214e-02, -1.6199e-01],\n",
      "        ...,\n",
      "        [ 1.8201e-01, -1.6528e-01, -1.6638e-01,  ..., -6.0913e-02,\n",
      "          3.9893e-01, -4.7394e-02],\n",
      "        [-1.3147e-01, -1.6968e-01, -6.2927e-02,  ..., -1.0144e-01,\n",
      "         -8.1177e-02, -7.3059e-02],\n",
      "        [ 4.4250e-02, -1.5686e-01, -9.2834e-02,  ..., -1.6406e-01,\n",
      "          8.0994e-02,  1.1688e-01]], device='cuda:0', dtype=torch.float16)\u001b[0m\u001b[1;34m\n",
      "        \u001b[0mencoder \u001b[1;34m= AutoEncoder()\u001b[0m\u001b[1;34m\n",
      "        \u001b[0m_ \u001b[1;34m= tensor([[[ 6.3125, -6.1367, -6.1406,  ..., -6.1445, -6.1406, -6.1406],\n",
      "         [ 4.3711, -5.4766, -5.4727,  ..., -5.5156, -5.4609, -5.4648],\n",
      "         [ 5.7266, -4.9961, -5.0156,  ..., -4.9805, -5.0078, -5.0039],\n",
      "         ...,\n",
      "         [ 5.2891, -4.4531, -4.5391,  ..., -4.5000, -4.5234, -4.5469],\n",
      "         [ 9.4844, -5.0547, -5.1367,  ..., -5.0625, -5.1172, -5.0781],\n",
      "         [ 1.8418, -5.6641, -5.6953,  ..., -5.7031, -5.6836, -5.7031]],\n",
      "\n",
      "        [[ 6.3125, -6.1367, -6.1406,  ..., -6.1445, -6.1406, -6.1406],\n",
      "         [ 4.2578, -5.7500, -5.7266,  ..., -5.7656, -5.7734, -5.7578],\n",
      "         [ 5.6211, -6.6172, -6.5312,  ..., -6.5938, -6.6016, -6.5703],\n",
      "         ...,\n",
      "         [ 5.3789, -8.0234, -7.9766,  ..., -7.9609, -7.9609, -7.9844],\n",
      "         [ 3.5156, -7.9844, -7.9766,  ..., -7.9453, -7.9531, -8.0625],\n",
      "         [ 4.0469, -7.7500, -7.7031,  ..., -7.6875, -7.7109, -7.7227]],\n",
      "\n",
      "        [[ 6.3125, -6.1367, -6.1406,  ..., -6.1445, -6.1406, -6.1406],\n",
      "         [ 6.9492, -4.9375, -4.9492,  ..., -4.9531, -4.9336, -4.9102],\n",
      "         [ 1.1035, -4.7812, -4.7422,  ..., -4.7656, -4.7656, -4.7812],\n",
      "         ...,\n",
      "         [ 5.2344, -5.7148, -5.6914,  ..., -5.7422, -5.7578, -5.7031],\n",
      "         [ 1.7607, -5.5156, -5.5664,  ..., -5.5156, -5.5156, -5.5156],\n",
      "         [ 3.4922, -5.0312, -5.0273,  ..., -5.0625, -5.0391, -4.9688]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 6.3125, -6.1367, -6.1406,  ..., -6.1445, -6.1406, -6.1406],\n",
      "         [ 5.0234, -5.5430, -5.5781,  ..., -5.5586, -5.5391, -5.4883],\n",
      "         [ 8.4688, -6.3516, -6.3281,  ..., -6.3047, -6.3984, -6.3516],\n",
      "         ...,\n",
      "         [ 3.7617, -6.6719, -6.6406,  ..., -6.5625, -6.6875, -6.6328],\n",
      "         [-1.2324, -5.1484, -5.1953,  ..., -5.1797, -5.2422, -5.2422],\n",
      "         [ 1.2988, -3.9062, -3.8242,  ..., -3.9512, -3.9062, -4.0469]],\n",
      "\n",
      "        [[ 6.3125, -6.1367, -6.1406,  ..., -6.1445, -6.1406, -6.1406],\n",
      "         [ 2.1914, -6.0898, -6.0703,  ..., -6.0781, -6.0938, -6.0781],\n",
      "         [ 4.5625, -7.6094, -7.6094,  ..., -7.5938, -7.6094, -7.5938],\n",
      "         ...,\n",
      "         [ 2.6914, -7.5000, -7.4219,  ..., -7.3828, -7.4531, -7.4570],\n",
      "         [-3.8691, -8.1094, -8.0078,  ..., -8.0312, -8.0078, -8.2031],\n",
      "         [16.4219, -6.9531, -6.9297,  ..., -6.8984, -6.8750, -6.9492]],\n",
      "\n",
      "        [[ 6.3125, -6.1367, -6.1406,  ..., -6.1445, -6.1406, -6.1406],\n",
      "         [ 6.7070, -5.5859, -5.6172,  ..., -5.6406, -5.6016, -5.6094],\n",
      "         [ 3.4551, -6.7500, -6.7891,  ..., -6.7656, -6.7656, -6.7305],\n",
      "         ...,\n",
      "         [ 2.9434, -4.2656, -4.3047,  ..., -4.2656, -4.2656, -4.2812],\n",
      "         [ 4.7109, -5.2812, -5.3125,  ..., -5.2500, -5.2773, -5.2148],\n",
      "         [ 3.9668, -4.8984, -4.9375,  ..., -4.8164, -4.8672, -4.8203]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>)\u001b[0m\n",
      "\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Linearization component\u001b[39;00m\n",
      "\u001b[0;32m     70\u001b[0m feature \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mW_enc[:, feature_idx]\n",
      "\n",
      "File \u001b[1;32m/opt/conda/envs/sprint/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(\n",
      "    self=AutoEncoder(),\n",
      "    *args=(tensor([[-1.6870e-01, -4.2419e-02, -3.3617e-05, ...1688e-01]], device='cuda:0', dtype=torch.float16),),\n",
      "    **kwargs={}\n",
      ")\u001b[0m\n",
      "\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "        self \u001b[1;34m= AutoEncoder()\u001b[0m\u001b[1;34m\n",
      "        \u001b[0margs \u001b[1;34m= (tensor([[-1.6870e-01, -4.2419e-02, -3.3617e-05,  ..., -8.9294e-02,\n",
      "         -6.6895e-02, -1.3208e-03],\n",
      "        [-1.6406e-01, -1.2999e-03, -8.0643e-03,  ..., -8.4961e-02,\n",
      "         -1.3098e-01, -1.3562e-01],\n",
      "        [ 2.3926e-01,  1.1310e-01, -9.0027e-02,  ..., -2.0432e-02,\n",
      "         -8.2214e-02, -1.6199e-01],\n",
      "        ...,\n",
      "        [ 1.8201e-01, -1.6528e-01, -1.6638e-01,  ..., -6.0913e-02,\n",
      "          3.9893e-01, -4.7394e-02],\n",
      "        [-1.3147e-01, -1.6968e-01, -6.2927e-02,  ..., -1.0144e-01,\n",
      "         -8.1177e-02, -7.3059e-02],\n",
      "        [ 4.4250e-02, -1.5686e-01, -9.2834e-02,  ..., -1.6406e-01,\n",
      "          8.0994e-02,  1.1688e-01]], device='cuda:0', dtype=torch.float16),)\u001b[0m\u001b[1;34m\n",
      "        \u001b[0mkwargs \u001b[1;34m= {}\u001b[0m\n",
      "\n",
      "File \u001b[1;32m/opt/conda/envs/sprint/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(\n",
      "    self=AutoEncoder(),\n",
      "    *args=(tensor([[-1.6870e-01, -4.2419e-02, -3.3617e-05, ...1688e-01]], device='cuda:0', dtype=torch.float16),),\n",
      "    **kwargs={}\n",
      ")\u001b[0m\n",
      "\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "        forward_call \u001b[1;34m= <bound method AutoEncoder.forward of AutoEncoder()>\u001b[0m\u001b[1;34m\n",
      "        \u001b[0margs \u001b[1;34m= (tensor([[-1.6870e-01, -4.2419e-02, -3.3617e-05,  ..., -8.9294e-02,\n",
      "         -6.6895e-02, -1.3208e-03],\n",
      "        [-1.6406e-01, -1.2999e-03, -8.0643e-03,  ..., -8.4961e-02,\n",
      "         -1.3098e-01, -1.3562e-01],\n",
      "        [ 2.3926e-01,  1.1310e-01, -9.0027e-02,  ..., -2.0432e-02,\n",
      "         -8.2214e-02, -1.6199e-01],\n",
      "        ...,\n",
      "        [ 1.8201e-01, -1.6528e-01, -1.6638e-01,  ..., -6.0913e-02,\n",
      "          3.9893e-01, -4.7394e-02],\n",
      "        [-1.3147e-01, -1.6968e-01, -6.2927e-02,  ..., -1.0144e-01,\n",
      "         -8.1177e-02, -7.3059e-02],\n",
      "        [ 4.4250e-02, -1.5686e-01, -9.2834e-02,  ..., -1.6406e-01,\n",
      "          8.0994e-02,  1.1688e-01]], device='cuda:0', dtype=torch.float16),)\u001b[0m\u001b[1;34m\n",
      "        \u001b[0mkwargs \u001b[1;34m= {}\u001b[0m\n",
      "\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32m~/mats-sprint/src/sprint/sae_tutorial.py:52\u001b[0m, in \u001b[0;36mAutoEncoder.forward\u001b[1;34m(\n",
      "    self=AutoEncoder(),\n",
      "    x=tensor([[-1.6870e-01, -4.2419e-02, -3.3617e-05, ...1688e-01]], device='cuda:0', dtype=torch.float16)\n",
      ")\u001b[0m\n",
      "\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n",
      "\u001b[1;32m---> 52\u001b[0m     x_cent \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mb_dec\u001b[49m\n",
      "        x \u001b[1;34m= tensor([[-1.6870e-01, -4.2419e-02, -3.3617e-05,  ..., -8.9294e-02,\n",
      "         -6.6895e-02, -1.3208e-03],\n",
      "        [-1.6406e-01, -1.2999e-03, -8.0643e-03,  ..., -8.4961e-02,\n",
      "         -1.3098e-01, -1.3562e-01],\n",
      "        [ 2.3926e-01,  1.1310e-01, -9.0027e-02,  ..., -2.0432e-02,\n",
      "         -8.2214e-02, -1.6199e-01],\n",
      "        ...,\n",
      "        [ 1.8201e-01, -1.6528e-01, -1.6638e-01,  ..., -6.0913e-02,\n",
      "          3.9893e-01, -4.7394e-02],\n",
      "        [-1.3147e-01, -1.6968e-01, -6.2927e-02,  ..., -1.0144e-01,\n",
      "         -8.1177e-02, -7.3059e-02],\n",
      "        [ 4.4250e-02, -1.5686e-01, -9.2834e-02,  ..., -1.6406e-01,\n",
      "          8.0994e-02,  1.1688e-01]], device='cuda:0', dtype=torch.float16)\u001b[0m\u001b[1;34m\n",
      "        \u001b[0mself \u001b[1;34m= AutoEncoder()\u001b[0m\n",
      "\u001b[0;32m     53\u001b[0m     acts \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x_cent \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_enc \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb_enc)\n",
      "\u001b[0;32m     54\u001b[0m     x_reconstruct \u001b[38;5;241m=\u001b[39m acts \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_dec \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb_dec\n",
      "\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (2048) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "\n",
      "Tokens shape: torch.Size([215402, 128]), dtype: torch.int64, device: cuda:0\n",
      "{'batch_size': 4096,\n",
      " 'beta1': 0.9,\n",
      " 'beta2': 0.99,\n",
      " 'buffer_batches': 12288,\n",
      " 'buffer_mult': 384,\n",
      " 'buffer_size': 1572864,\n",
      " 'd_mlp': 2048,\n",
      " 'dict_mult': 8,\n",
      " 'enc_dtype': 'fp32',\n",
      " 'l1_coeff': 0.0003,\n",
      " 'lr': 0.0001,\n",
      " 'model_batch_size': 512,\n",
      " 'num_tokens': 2000000000,\n",
      " 'seed': 52,\n",
      " 'seq_len': 128}\n",
      "Encoder device: cuda:0\n",
      "Loaded pretrained model gelu-1l into HookedTransformer\n",
      "Moving model to device:  cuda\n",
      "Changing model dtype to torch.float16\n",
      "Model device: cuda:0\n",
      "Tokens shape: torch.Size([215402, 128]), dtype: torch.int64, device: cuda:0\n",
      "-4.359375\n",
      "Loaded pretrained model gelu-1l into HookedTransformer\n",
      "Moving model to device:  cuda\n",
      "Changing model dtype to torch.float16\n",
      "Model device: cuda:0\n",
      "Loaded pretrained model gelu-1l into HookedTransformer\n",
      "Moving model to device:  cuda\n",
      "Changing model dtype to torch.float16\n",
      "Model device: cuda:0\n",
      "Tokens shape: torch.Size([215402, 128]), dtype: torch.int64, device: cuda:0\n",
      "-4.359375\n",
      "Loaded pretrained model gelu-1l into HookedTransformer\n",
      "Moving model to device:  cuda\n",
      "Changing model dtype to torch.float16\n",
      "Model device: cuda:0\n",
      "Tokens shape: torch.Size([215402, 128]), dtype: torch.int64, device: cuda:0\n",
      "{'batch_size': 4096,\n",
      " 'beta1': 0.9,\n",
      " 'beta2': 0.99,\n",
      " 'buffer_batches': 12288,\n",
      " 'buffer_mult': 384,\n",
      " 'buffer_size': 1572864,\n",
      " 'd_mlp': 2048,\n",
      " 'dict_mult': 8,\n",
      " 'enc_dtype': 'fp32',\n",
      " 'l1_coeff': 0.0003,\n",
      " 'lr': 0.0001,\n",
      " 'model_batch_size': 512,\n",
      " 'num_tokens': 2000000000,\n",
      " 'seed': 52,\n",
      " 'seq_len': 128}\n",
      "Encoder device: cuda:0\n",
      "Tokens shape: torch.Size([215402, 128]), dtype: torch.int64, device: cuda:0\n",
      "-4.359375\n"
     ]
    }
   ],
   "source": [
    "from sprint.loading import load_sae\n",
    "\n",
    "load_sae(run_id=\"l1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sprint",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
