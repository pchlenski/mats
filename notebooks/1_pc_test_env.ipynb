{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world\n"
     ]
    }
   ],
   "source": [
    "# Test kernel\n",
    "print(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n"
     ]
    }
   ],
   "source": [
    "# Test numpy\n",
    "import numpy as np\n",
    "\n",
    "a = np.array([1, 2, 3])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7885, 0.3010, 0.8253],\n",
      "        [0.8887, 0.7543, 0.4657],\n",
      "        [0.7037, 0.7670, 0.9824],\n",
      "        [0.6595, 0.8532, 0.6683],\n",
      "        [0.1593, 0.9059, 0.1298]])\n"
     ]
    }
   ],
   "source": [
    "# Test torch\n",
    "import torch\n",
    "\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "0\n",
      "NVIDIA GeForce RTX 4090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 199017.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# Test torch CUDA usage - run nvidia_smi in parallel\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())  # Should be 0\n",
    "print(torch.cuda.get_device_name(0))  # Should be RTX 4090\n",
    "\n",
    "# Do something to keep the GPU busy\n",
    "a = torch.rand(10000, 10000).cuda()\n",
    "b = torch.rand(10000, 10000).cuda()\n",
    "\n",
    "for i in tqdm(range(1000)):\n",
    "    a = torch.matmul(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint\n"
     ]
    }
   ],
   "source": [
    "print(\"Checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gelu-1l into HookedTransformer\n",
      "Moving model to device:  cuda\n",
      "Changing model dtype to torch.float16\n",
      "Model device: cuda:0\n",
      "Tokens shape: torch.Size([215402, 128]), dtype: torch.int32, device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Transformer lens loading\n",
    "\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.utils import tokenize_and_concatenate\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Get model\n",
    "model = HookedTransformer.from_pretrained(\"gelu-1l\")\n",
    "model = model.cuda()\n",
    "model = model.to(torch.float16)\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "\n",
    "# Get and tokenize data\n",
    "data = load_dataset(\"NeelNanda/c4-code-20k\", split=\"train\")\n",
    "tokenized_data = tokenize_and_concatenate(data, model.tokenizer, max_length=128).shuffle(42)\n",
    "tokens = tokenized_data[\"tokens\"]\n",
    "tokens = tokens.cuda()\n",
    "tokens = tokens.to(torch.int32)\n",
    "print(f\"Tokens shape: {tokens.shape}, dtype: {tokens.dtype}, device: {tokens.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:09<00:00, 10.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([3200, 128, 48262]), dtype: torch.float16, device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Run model\n",
    "tokens_batched = tokens.split(32)\n",
    "out = []\n",
    "for batch in tqdm(tokens_batched[:100]):\n",
    "    out.append(model(batch).detach().cpu())  # Move to CPU for memory\n",
    "\n",
    "out = torch.cat(out)\n",
    "print(f\"Output shape: {out.shape}, dtype: {out.dtype}, device: {out.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
