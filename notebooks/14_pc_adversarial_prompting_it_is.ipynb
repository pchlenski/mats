{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial prompting for 'it is' feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_IDX = 4542"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gelu-1l into HookedTransformer\n",
      "Moving model to device:  cuda\n",
      "Changing model dtype to torch.float16\n",
      "Model device: cuda:0\n",
      "Tokens shape: torch.Size([215402, 128]), dtype: torch.int64, device: cuda:0\n",
      "Loading run1 from HuggingFace at 25\n",
      "{'batch_size': 4096,\n",
      " 'beta1': 0.9,\n",
      " 'beta2': 0.99,\n",
      " 'buffer_batches': 12288,\n",
      " 'buffer_mult': 384,\n",
      " 'buffer_size': 1572864,\n",
      " 'd_mlp': 2048,\n",
      " 'dict_mult': 8,\n",
      " 'enc_dtype': 'fp32',\n",
      " 'l1_coeff': 0.0003,\n",
      " 'lr': 0.0001,\n",
      " 'model_batch_size': 512,\n",
      " 'num_tokens': 2000000000,\n",
      " 'seed': 52,\n",
      " 'seq_len': 128}\n",
      "Encoder device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from transformer_lens import utils\n",
    "from sprint.loading import load_all\n",
    "from sprint.vars import NEWLINE\n",
    "\n",
    "model, data, sae = load_all(fold_ln=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def does_feature_fire(prompt, feature=FEATURE_IDX, model=model, sae=sae, layer=0):\n",
    "#     act_name = utils.get_act_name(\"post\", layer)\n",
    "#     model_out, cache = model.run_with_cache(prompt, names_filter=[act_name])\n",
    "#     # fires =  (sae(cache[act_name])[2][0, :, feature] > 0).any()\n",
    "#     pre_relu = (sae.W_enc @ cache[act_name] + sae.b_enc)[0, :, feature]\n",
    "#     return cache[act_name]\n",
    "\n",
    "#     # return pre_relu\n",
    "#     return (cache[act_name] @ sae.W_enc + sae.b_enc)[0, :, FEATURE_IDX]\n",
    "\n",
    "\n",
    "def get_feature_scores_for_prompt(\n",
    "    prompt, model=model, encoder=sae, feature=FEATURE_IDX, act_name=\"post\", layer=0, prepend_bos=True, bias=False\n",
    "):\n",
    "    with torch.no_grad():\n",
    "        _, cache = model.run_with_cache(\n",
    "            prompt, names_filter=[utils.get_act_name(act_name, layer)], prepend_bos=prepend_bos\n",
    "        )\n",
    "        acts = cache[utils.get_act_name(act_name, layer)]\n",
    "        acts = acts.reshape(-1, encoder.W_enc.shape[0])\n",
    "    out = (acts - encoder.b_dec) @ encoder.W_enc[:, feature]\n",
    "    if bias:\n",
    "        out += encoder.b_enc[feature]\n",
    "    return utils.to_numpy(out)\n",
    "\n",
    "    # loss, x_reconstruct, hidden_acts, l2_loss, l1_loss = encoder(acts)\n",
    "    # return utils.to_numpy(hidden_acts[:, feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full stop. It is in your best interest to\t\tTrue\t\tTrue\t\t1.73\n",
      "Full stop. It intercepted in your best interest\t\tFalse\t\tFalse\t\t-1.064\n",
      "The quick brown fox jumps over the lazy dog\t\tFalse\t\tFalse\t\t-1.477\n",
      "Enough about the fox. It's time to talk dog\t\tFalse\t\tFalse\t\t-0.8643\n",
      "I have intercepted a message from the fox\t\tFalse\t\tFalse\t\t-1.393\n",
      "redirect what is left of the fox to the dog\t\tFalse\t\tTrue\t\t-1.557\n",
      "redirect intercepted materially the antit unequivocally\t\tFalse\t\tFalse\t\t-1.372\n",
      "Stop.↩It intercepted.↩Stop. It really should be\t\tFalse\t\tFalse\t\t-0.911\n",
      "Stop.↩redirect.↩Stop. It really should\t\tFalse\t\tFalse\t\t-0.9434\n",
      "To be or not to be, that is the question\t\tFalse\t\tTrue\t\t-0.251\n",
      "They don't think it be like it is, but it do\t\tTrue\t\tTrue\t\t0.8535\n",
      "They don't think it be like it be, but it do\t\tFalse\t\tFalse\t\t-0.6367\n",
      "They don't think it be like It be but it do\t\tFalse\t\tFalse\t\t-0.4922\n",
      "They don't think it be like.↩ It be, but it do\t\tFalse\t\tFalse\t\t-0.2478\n",
      " intercepted it be.↩ It be.↩ It be.↩ It be it be\t\tTrue\t\tFalse\t\t0.062\n",
      " it be.↩ It be.↩ It be.↩ It be it be\t\tTrue\t\tFalse\t\t0.2097\n",
      " it be.↩ It be.↩ It be.↩ It be.↩ It be.↩ It be.\t\tTrue\t\tFalse\t\t0.1274\n",
      " it be.↩ It be.↩ It be.↩ It be\t\tTrue\t\tFalse\t\t0.1138\n",
      " it be.↩ It be.\t\tTrue\t\tFalse\t\t0.0105\n",
      "It pertains comes.↩ Suffice to say, it be\t\tTrue\t\tFalse\t\t0.083\n",
      "Get the best value for your business.↩ It's also advisable that when looking\t\tFalse\t\tFalse\t\t-0.2861\n",
      "Get the best value for your business.↩ It would also be advisable that when looking\t\tFalse\t\tFalse\t\t-0.579\n",
      " it was.↩ It was.\t\tFalse\t\tFalse\t\t-0.8076\n",
      "not bear the responsibility of the task, and he is the backbone of the family\t\tTrue\t\tTrue\t\t0.0752\n"
     ]
    }
   ],
   "source": [
    "for prompt in [\n",
    "    \"Full stop. It is in your best interest to\",\n",
    "    \"Full stop. It intercepted in your best interest\",\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"Enough about the fox. It's time to talk dog\",\n",
    "    \"I have intercepted a message from the fox\",\n",
    "    \"redirect what is left of the fox to the dog\",\n",
    "    \"redirect intercepted materially the antit unequivocally\",\n",
    "    \"Stop.\\nIt intercepted.\\nStop. It really should be\",\n",
    "    \"Stop.\\nredirect.\\nStop. It really should\",\n",
    "    \"To be or not to be, that is the question\",\n",
    "    \"They don't think it be like it is, but it do\",\n",
    "    \"They don't think it be like it be, but it do\",\n",
    "    \"They don't think it be like It be but it do\",\n",
    "    \"They don't think it be like.\\n It be, but it do\",\n",
    "    \" intercepted it be.\\n It be.\\n It be.\\n It be it be\",\n",
    "    \" it be.\\n It be.\\n It be.\\n It be it be\",\n",
    "    \" it be.\\n It be.\\n It be.\\n It be.\\n It be.\\n It be.\",\n",
    "    \" it be.\\n It be.\\n It be.\\n It be\",\n",
    "    \" it be.\\n It be.\",\n",
    "    \"It pertains comes.\\n Suffice to say, it be\",\n",
    "    \"Get the best value for your business.\\n It's also advisable that when looking\",\n",
    "    \"Get the best value for your business.\\n It would also be advisable that when looking\",\n",
    "    \" it was.\\n It was.\",\n",
    "    \"not bear the responsibility of the task, and he is the backbone of the family\"\n",
    "]:\n",
    "    # feat_score = does_feature_fire(prompt)\n",
    "    # print(f\"{prompt}\\t\\t{feat_score > 0}\\t\\t{feat_score}\")\n",
    "\n",
    "    feat_score = get_feature_scores_for_prompt(prompt, bias=True)\n",
    "    print(\n",
    "        prompt.replace(\"\\n\", NEWLINE),\n",
    "        (feat_score > 0).any(),\n",
    "        # np.where(feat_score > 0)[0],\n",
    "        # np.where(model.to_tokens(prompt).cpu() == model.to_single_token(\" is\"))[1],\n",
    "        model.to_tokens(prompt).cpu()[0, np.argmax(feat_score)].item() == model.to_single_token(\" is\"),\n",
    "        np.max(feat_score),\n",
    "        sep=\"\\t\\t\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2717, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.cosine_similarity(\n",
    "    model.W_E[model.to_single_token(\" is\")], model.W_E[model.to_single_token(\"Is\")], dim=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2495, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.cosine_similarity(\n",
    "    model.W_E[model.to_single_token(\" is\")], model.W_E[model.to_single_token(\"\")], dim=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where does .2097 rank?\n",
    "\n",
    "from sprint.feature_exploration import max_activating_examples\n",
    "\n",
    "max_examples = max_activating_examples(model=model, data=data, feature_id=FEATURE_IDX, n_examples=1000, evenly_spaced=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(94.9000, device='cuda:0')"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(max_examples[1].max(axis=1)[0] > 0.2097).sum() / 1000 * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sprint",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
